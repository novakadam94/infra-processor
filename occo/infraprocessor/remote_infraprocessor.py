#
# Copyright (C) 2014 MTA SZTAKI
#

""" Infrastructure Processor for OCCO

.. moduleauthor:: Adam Visegradi <adam.visegradi@sztaki.mta.hu>

.. ifconfig:: api_doc is False

    .. autoclass:: RemotePushStrategy
        :members:
"""

__all__ = ['RemoteInfraProcessor', 'RemoteInfraProcessorSkeleton',
           'Mgmt_SkipUntil']

import logging
import occo.util.communication as comm
import occo.util.factory as factory
from occo.infraprocessor.strategy import Strategy
from occo.infraprocessor.basic_infraprocessor \
    import Command, BasicInfraProcessor, InfraProcessor
import time

log = logging.getLogger('occo.infraprocessor.remote')

@factory.register(Strategy, 'remote')
class RemotePushStrategy(Strategy):
    """
    Implements :class:`Strategy` by simply pushing the instruction list to
    another infrastructure processor. I.e.: to the "real" infrastructure
    processor.

    :param destination_queue: The communication channel to where the instruction
        shall be pushed.
    :type destination_queue:
        :class:`occo.util.communication.comm.AsynchronProducer`

    .. todo:: *IMPORTANT.* Currently, this strategy pushes instructions
        individually---it splits the parallelizable instruction list into
        pieces that will always be executed sequentially. The batch *must* be
        kept together. I recall I've already solved this, but I don't know what
        happened with it :S --Adam
    """
    def __init__(self, destination_queue_cfg):
        super(RemotePushStrategy, self).__init__()
        self.queue_cfg = destination_queue_cfg
        self.queue = comm.AsynchronProducer.instantiate(
            **destination_queue_cfg)
    def perform(self, infraprocessor, instruction_list):
        with self.queue:
            #TODO push as list; keep instructions together
            results = list()
            for i in instruction_list:
                if self.cancelled:
                    break
                result = self.queue.push_message(i)
                results.append(result)
            return results

class Mgmt_SkipUntil(Command):
    """
    Implementation of cancelling commands until a given time.

    :param int deadline: Unix timestamp that will be compared to
        :class:`Command`-s ``timestamp``.

    Performing this will essentially clean the queue and the Infrastructure
    Processor of pending instructions.

    The Infrastructure Processor will *try* to abort and undo instructions
    already being executed. Otherwise, it will wait until they are finalized.

    The Infrastructure Processor will then disregard new instructions up to the
    given deadline (based on their ``timestamp``).

    This instruction is meant to be delivered through the *synchronous*
    management queue of the :class:`RemoteInfraProcessor`. When this
    instruction has been executed, the Infrastructure Processor will be idle,
    and the queue will be virtually empty---the infrastructure will be in a
    non-transient state. Thus, instructions generated by the :ref:`Enactor
    <enactor>` at this point will be executed without interference.

    .. todo:: This feature will probably not work as-is, because the backend IP
        will have to know about it being used as a backend. This Command should
        be part of (defined and created (cri) by) the backends.

    """
    def __init__(self, deadline):
        Command.__init__(self)
        self.deadline = deadline
    def perform(self, infraprocessor):
        infraprocessor.cancel_upcoming(self.deadline)

@factory.register(InfraProcessor, 'remote')
class RemoteInfraProcessor(BasicInfraProcessor):
    """
    A remote implementation of :class:`InfraProcessor`.

    The exact same command objects are created by this class as that by the
    :class:`BasicInfraProcessor`. The difference is that this class uses the
    :class:`RemotePushStrategy` to perform instructions.

    This class communicates with a :class:`RemoteInfraProcessorSkeleton`
    through a :class:`~occo.util.communication.comm.AsynchronProducer`.

    :param destination_queue_cfg: The configuration for the backend
        communication channel.

    .. todo:: Rethink queue context management.
        See :meth:`InfraProcessor.__enter__`.
    """
    def __init__(self, destination_queue_cfg):
        # Calling only the abstract IP's __init__
        # (and skipping BasicInfraProcessor.__init__) is intentional:
        #
        # Command classes must be inherited (hence the BasicInfraProcessor
        # parent), but this class does not need the IP's backends (infobroker,
        # cloudhandler, etc.)
        InfraProcessor.__init__(
            self,
            process_strategy=dict(
                protocol='remote',
                args=(destination_queue_cfg,)))

    def cancel_pending(self, deadline):
        """
        Implementation of :meth:`InfraProcessor.cancel_pending` with
        :class:`Mgmt_SkipUntil`.
        """
        self.push_instructions([Mgmt_SkipUntil(deadline)])

class RemoteInfraProcessorSkeleton(object):
    """
    Skeleton_ part of the Infrastructure Processor RMI solution.

    A ~ object reads serialized :class:`Command`\ s from the backend queue,
    deserializes them, and dispatches them to the backend ("real")
    infrastructure processor. These commands are sent by a client application
    through a :class:`RemoteInfraProcessor` object.

    :param backend_ip: The "real" *I*\ nfrastructure *P*\ rocessor (hence
        ``_ip``).  Actually, this may be another :class:`RemoteInfraProcessor`
        if necessary for some twisted reason...
    :type backend_ip: :class:`InfraProcessor`

    :param ip_queue_cfg: Configuration intended for the backend
        :class:`~occo.util.communication.comm.EventDrivenConsumer` for the
        *regular* instruction queue. This queue is expected to be an
        asynchronous message queue.

    :param control_queue_cfg: Configuration intended for the backend
        :class:`~occo.util.communication.comm.EventDrivenConsumer` for the
        *management* instruction queue. This queue is expected to be an RMI
        queue.

    :param cancel_event: Event object used to abort processing. If :data:`None`,
        :meth:`start_consuming` will yield immediately after polling both
        queues once. In this case, it must be called by the client repeatedly.
    :type cancel_event: :class:`threading.Event`

    .. _skeleton:
        http://stackoverflow.com/questions/8586206/what-is-stub-on-the-server-and-what-does-skeleton-mean
    """
    def __init__(self, backend_ip, ip_queue_cfg, control_queue_cfg,
                 cancel_event=None):
        self.backend_ip = backend_ip
        self.cancel_event = cancel_event

        # Ensure that these consumers are non-looping.
        # This is because polling has to alternate on them. If one of them is
        # looping, the other will never be polled.
        ip_queue_cfg['cancel_event'] = None
        control_queue_cfg['cancel_event'] = None

        self.ip_consumer = comm.EventDrivenConsumer.instantiate(
            processor=self.process_ip_msg, **ip_queue_cfg)
        self.control_consumer = comm.EventDrivenConsumer.instantiate(
            processor=self.process_control_msg, **control_queue_cfg)

    def __enter__(self):
        # Start the contexts of the consumers transactionally
        self.ip_consumer.__enter__()
        try:
            self.control_consumer.__enter__()
        except:
            # This is a "ROLLBACK"
            self.control_consumer.__exit__(None, None, None)
            raise
        return self
    def __exit__(self, type, value, tb):
        try:
            self.ip_consumer.__exit__(type, value, tb)
        finally:
            self.control_consumer.__exit__(type, value, tb)

    @property
    def cancelled(self):
        """ Returns true iff :meth:`start_consuming` should yield. """
        return self.cancel_event is None or self.cancel_event.is_set()

    def start_consuming(self):
        """
        Starts processing the input queues.

        This method will loop if ``self.cancel_event`` is specified, until a
        signal arrives through this object.

        If ``self.cancel_event`` is :data:`None`, this method will yield after
        polling both queues.

        The control queue is polled first so urgent control messages---like,
        e.g. :class:`Mgmt_SkipUntil`---arrive before starting work needlessly.
        """
        while not self.cancelled:
            log.debug("Processing control messages")
            self.control_consumer.start_consuming()
            log.debug("Processing normal messages")
            self.ip_consumer.start_consuming()
            time.sleep(0) # Yield CPU

    def process_ip_msg(self, instruction_list, *args, **kwargs):
        """ Callback function for the regular queue. """
        # Return value not needed -- this is NOT an rpc queue
        log.debug("Received normal message")
        self.backend_ip.push_instructions(instruction_list)

    def process_control_msg(self, instruction, *args, **kwargs):
        """ Callback function for the control queue. """
        # This is an RPC queue.
        # Control messages are immediately performed, disregarding
        # their timestamp and skip_until.
        log.debug("Received control message")
        try:
            retval = instruction.perform(self.backend_ip)
            return comm.Response(200, retval)
        except Exception as ex:
            return comm.ExceptionResponse(500, ex)
